{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SHL_split_and_batch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0qqr33La78CO1LXfSXiWA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhanfuadabir/SHL2020/blob/master/SHL_split_and_batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ad_Fv9HrO85",
        "outputId": "e6948287-9d54-4fea-9174-1d1aeb7fbbea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhjWE6Spq09X"
      },
      "source": [
        "def createBatch(data, batch_size = 1000, random_state=None):\n",
        "\n",
        "  \"\"\"\n",
        "  Randomly selects `batch_size` entries of each label from the dataset \n",
        "\n",
        "  Parameters:\n",
        "\n",
        "      data          : The dataset to create small batch from. \n",
        "                      This must be a DataFrame.\n",
        "      batch_size    : Batch size of each label\n",
        "      random_state  : Seed for the random number generator (if int), or numpy \n",
        "                      RandomState object.\n",
        "  \n",
        "  Returns:\n",
        "\n",
        "      Series or DataFrame\n",
        "  \"\"\"\n",
        "\n",
        "  y = data.label\n",
        "  num_label = y.nunique()\n",
        "  newDataBatch = pd.DataFrame(columns=data.columns)\n",
        "  \n",
        "  for i in range(1, num_label + 1):\n",
        "    data_i = data.loc[y == i, :].sample(n=batch_size, random_state=random_state)\n",
        "    newDataBatch = pd.concat([newDataBatch, data_i], axis=0)\n",
        "  return newDataBatch\n",
        "\n",
        "\n",
        "def random_split_half(data, random_state=123):\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y = data.label\n",
        "  num_label = y.nunique()\n",
        "  newDataBatch1 = pd.DataFrame(columns=data.columns)\n",
        "  \n",
        "  for i in range(1, num_label + 1):\n",
        "    data_i = data.loc[y == i, :].sample(frac=0.5, random_state=random_state)\n",
        "    newDataBatch1 = pd.concat([newDataBatch1, data_i], axis=0)\n",
        "  newDataBatch2 = pd.concat([data, newDataBatch1], axis=0)\n",
        "  newDataBatch2 = newDataBatch2.drop_duplicates(keep=False)\n",
        "  return newDataBatch1, newDataBatch2\n",
        "\n",
        "\n",
        "def process_train_validation(trainSet, valSet=None, trainBatch=1000,\n",
        "                             splitValSet=False, random_state=1234, \n",
        "                             removeConstantColumn=True, scaleFeatures=True,\n",
        "                             noTestLabel=False):\n",
        "\n",
        "  import pandas as pd\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "  print('Given Train Set Shape: ', trainSet.shape)\n",
        "  print('Given Validation Set Shape: ', valSet.shape)\n",
        "\n",
        "  if trainBatch != None:\n",
        "    print('\\nCreating train batch...', end=' ')\n",
        "    trainSet = createBatch(trainSet, batch_size=trainBatch, \n",
        "                          random_state=random_state)\n",
        "    print('Done | Shape: ', trainSet.shape)\n",
        "\n",
        "  if splitValSet == True:\n",
        "    print('Merging train batch with half validation set...', end=' ')\n",
        "    valTrain, valSet = random_split_half(valSet, random_state=random_state)\n",
        "    trainSet = pd.concat([trainSet, valTrain], axis = 0)\n",
        "    print('Done | Shape: ', trainSet.shape)\n",
        "\n",
        "  X_train = trainSet.drop('label', axis=1)\n",
        "  y_train = trainSet.label\n",
        "  if noTestLabel == False:\n",
        "    X_val = valSet.drop('label', axis=1)\n",
        "    y_val = valSet.label\n",
        "  else:\n",
        "    X_val = valSet\n",
        "\n",
        "  if removeConstantColumn == True:\n",
        "    X_temp = X_train\n",
        "    X_train = X_train.loc[:, (X_temp != X_temp.iloc[0]).any()]\n",
        "    X_val = X_val.loc[:, (X_temp != X_temp.iloc[0]).any()]\n",
        "  \n",
        "  if scaleFeatures == True:\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.fit_transform(X_val)\n",
        "\n",
        "  if noTestLabel == False:\n",
        "    print('\\nX_train Shape: ', X_train.shape, ' |  y_train Shape: ', y_train.shape)\n",
        "    print('X_val Shape: ', X_val.shape, ' |  y_val Shape: ', y_val.shape)\n",
        "\n",
        "    print('\\nInstances of each Label of the Train Set: ')\n",
        "    print(y_train.value_counts())\n",
        "\n",
        "    print('\\nInstances of each Label of the Validation Set: ')\n",
        "    print(y_val.value_counts())\n",
        "\n",
        "    return X_train, y_train, X_val, y_val\n",
        "  \n",
        "  else:\n",
        "    print('\\nX_train Shape: ', X_train.shape, ' |  y_train Shape: ', y_train.shape)\n",
        "    print('X_val Shape: ', X_val.shape)\n",
        "\n",
        "    print('\\nInstances of each Label of the Train Set: ')\n",
        "    print(y_train.value_counts())\n",
        "\n",
        "    return X_train, y_train, X_val\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USfCBEJarigi",
        "outputId": "5fa29be1-0e07-4217-bed2-a2bd9008a5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from joblib import load, dump\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "path = '/content/drive/My Drive/SHL Features Pickle/'\n",
        "train1Prefix = 'validation_2019'\n",
        "train2Prefix = 'validation_2020'\n",
        "train3Prefix = 'test_2019'\n",
        "testPrefix = 'test_2020'\n",
        "valPrefix = 'train_2020'\n",
        "positions = ['_hand', '_bag', '_torso', '_hips']\n",
        "position_val = ['_hand', '_hips']\n",
        "\n",
        "data_train1 = pd.DataFrame()\n",
        "for pos in positions:\n",
        "  #Unpickle Train1 Set\n",
        "  print('Unpickling from: ' + train1Prefix + pos + '_DATA.pickle ...',end=' ')\n",
        "  temp = pd.read_pickle(path + train1Prefix + pos + '_DATA.pickle')\n",
        "  print('Done | Shape: ', temp.shape)\n",
        "  data_train1 = data_train1.append(temp,ignore_index=True)\n",
        "print(train1Prefix, ' shape: ', data_train1.shape)\n",
        "\n",
        "\n",
        "data_train2 = pd.DataFrame()\n",
        "for pos in positions:\n",
        "  #Unpickle Train2 Set\n",
        "  print('Unpickling from: ' + train2Prefix + pos + '_DATA.pickle ...',end=' ')\n",
        "  temp = pd.read_pickle(path + train2Prefix + pos + '_DATA.pickle')\n",
        "  print('Done | Shape: ', temp.shape)\n",
        "  data_train2 = data_train2.append(temp,ignore_index=True)\n",
        "print(train2Prefix, ' shape: ', data_train2.shape)\n",
        "\n",
        "\n",
        "#Unpickle Train3 Set\n",
        "print('Unpickling from: ' + train3Prefix + '_hand' + '_DATA.pickle ...',end=' ')\n",
        "data_train3 = pd.read_pickle(path + train3Prefix + '_hand' + '_DATA.pickle')\n",
        "print('Done | Shape: ', data_train3.shape)\n",
        "print(train3Prefix, ' shape: ', data_train3.shape)\n",
        "\n",
        "data_train = pd.concat([data_train1, data_train2, data_train3], axis=0)\n",
        "print('\\n\\ndata_train shape: ', data_train.shape, end='\\n\\n')\n",
        "\n",
        "data_val = pd.DataFrame()\n",
        "for pos in position_val:\n",
        "  #Unpickle Validation Set\n",
        "  print('Unpickling from: ' + valPrefix + pos + '_DATA.pickle ...',end=' ')\n",
        "  temp = pd.read_pickle(path + valPrefix + pos + '_DATA.pickle')\n",
        "  print('Done | Shape: ', temp.shape)\n",
        "  data_val = data_val.append(temp,ignore_index=True)\n",
        "print(valPrefix, ' shape: ', data_val.shape)\n",
        "\n",
        "\n",
        "#Unpickle Test Set\n",
        "print('Unpickling from: ' + testPrefix + '_hand' + '_DATA.pickle ...',end=' ')\n",
        "data_test = pd.read_pickle(path + testPrefix + '_hand' + '_DATA.pickle')\n",
        "print('Done | Shape: ', data_test.shape)\n",
        "\n",
        "\n",
        "# Check for nan\n",
        "if data_train.isna().any().any() == True:\n",
        "  print('\\nnan Detected in Train set')\n",
        "  # Drop nan rows\n",
        "  print('Dropping nan rows...',end=' ')\n",
        "  data_train.dropna(inplace=True)\n",
        "  print('Done | Shape: ', data_train.shape)\n",
        "\n",
        "if data_val.isna().any().any() == True:\n",
        "  print('\\nnan Detected in Validation set')\n",
        "  # Drop nan rows\n",
        "  print('Dropping nan rows...',end=' ')\n",
        "  data_val.dropna(inplace=True)\n",
        "  print('Done | Shape: ', data_val.shape)\n",
        "\n",
        "if data_test.isna().any().any() == True:\n",
        "  print('\\nnan Detected in Test set')\n",
        "  # Drop nan rows\n",
        "  print('Dropping nan rows...',end=' ')\n",
        "  data_test.dropna(inplace=True)\n",
        "  print('Done | Shape: ', data_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Unpickling from: validation_2019_hand_DATA.pickle ... Done | Shape:  (12177, 496)\n",
            "Unpickling from: validation_2019_bag_DATA.pickle ... Done | Shape:  (12177, 496)\n",
            "Unpickling from: validation_2019_torso_DATA.pickle ... Done | Shape:  (12177, 496)\n",
            "Unpickling from: validation_2019_hips_DATA.pickle ... Done | Shape:  (12177, 496)\n",
            "validation_2019  shape:  (48708, 496)\n",
            "Unpickling from: validation_2020_hand_DATA.pickle ... Done | Shape:  (28789, 496)\n",
            "Unpickling from: validation_2020_bag_DATA.pickle ... Done | Shape:  (28789, 496)\n",
            "Unpickling from: validation_2020_torso_DATA.pickle ... Done | Shape:  (28789, 496)\n",
            "Unpickling from: validation_2020_hips_DATA.pickle ... Done | Shape:  (28789, 496)\n",
            "validation_2020  shape:  (115156, 496)\n",
            "Unpickling from: test_2019_hand_DATA.pickle ... Done | Shape:  (55811, 496)\n",
            "test_2019  shape:  (55811, 496)\n",
            "\n",
            "\n",
            "data_train shape:  (219675, 496)\n",
            "\n",
            "Unpickling from: train_2020_hand_DATA.pickle ... Done | Shape:  (196072, 496)\n",
            "Unpickling from: train_2020_hips_DATA.pickle ... Done | Shape:  (196072, 496)\n",
            "train_2020  shape:  (392144, 496)\n",
            "Unpickling from: test_2020_hand_DATA.pickle ... Done | Shape:  (57573, 495)\n",
            "\n",
            "nan Detected in Validation set\n",
            "Dropping nan rows... Done | Shape:  (392143, 496)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX8qKc-iroYR",
        "outputId": "ba6378f4-9c59-4288-a35d-63a5e581bf03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "X_train, y_train, X_test, y_test = process_train_validation(data_train2, data_val, \n",
        "                                                            trainBatch=None, \n",
        "                                                            splitValSet=False, \n",
        "                                                            random_state=1234)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given Train Set Shape:  (115156, 496)\n",
            "Given Validation Set Shape:  (392143, 496)\n",
            "\n",
            "X_train Shape:  (115156, 452)  |  y_train Shape:  (115156,)\n",
            "X_val Shape:  (392143, 452)  |  y_val Shape:  (392143,)\n",
            "\n",
            "Instances of each Label of the Train Set: \n",
            "1.0    23868\n",
            "2.0    20900\n",
            "7.0    17448\n",
            "8.0    17368\n",
            "5.0    16380\n",
            "4.0     9628\n",
            "6.0     7344\n",
            "3.0     2220\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Instances of each Label of the Validation Set: \n",
            "5.0    63466\n",
            "7.0    62506\n",
            "6.0    56652\n",
            "2.0    49088\n",
            "1.0    48912\n",
            "8.0    47668\n",
            "4.0    46946\n",
            "3.0    16905\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4PxS8X2r0Ks",
        "outputId": "bc7665a5-5ac0-4b6d-9a22-08ad272cae20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "import time\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=300,verbose=True,n_jobs=-1)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate Algorithm\n",
        "print(\"\\n\\nConfusion Matrix: \\n\\n\",confusion_matrix(y_test,y_pred))\n",
        "print(\"\\n\\nReport: \\n\\n\",classification_report(y_test,y_pred))\n",
        "print(\"Accuracy: \",accuracy_score(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 10.6min\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 16.2min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    3.9s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   16.2s\n",
            "[Parallel(n_jobs=2)]: Done 300 out of 300 | elapsed:   24.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Confusion Matrix: \n",
            "\n",
            " [[44880   649     0    98   316    58  1314  1597]\n",
            " [ 1952 44469   113  1685   229   139   114   387]\n",
            " [   26  9004  7854    11     8     0     2     0]\n",
            " [ 1570 24121   939 15715  1736  2000    90   775]\n",
            " [ 4069   458     0   487 15313 20906 15909  6324]\n",
            " [ 6960   409     0   153  6237 20392 16576  5925]\n",
            " [ 9831   504     0    71  1015   396 29755 20934]\n",
            " [ 4413   220     0    29   744   120  9275 32867]]\n",
            "\n",
            "\n",
            "Report: \n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.61      0.92      0.73     48912\n",
            "         2.0       0.56      0.91      0.69     49088\n",
            "         3.0       0.88      0.46      0.61     16905\n",
            "         4.0       0.86      0.33      0.48     46946\n",
            "         5.0       0.60      0.24      0.34     63466\n",
            "         6.0       0.46      0.36      0.41     56652\n",
            "         7.0       0.41      0.48      0.44     62506\n",
            "         8.0       0.48      0.69      0.56     47668\n",
            "\n",
            "    accuracy                           0.54    392143\n",
            "   macro avg       0.61      0.55      0.53    392143\n",
            "weighted avg       0.57      0.54      0.51    392143\n",
            "\n",
            "Accuracy:  0.5386937928255764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enJ2hkMxtIw7",
        "outputId": "63d62f4f-901f-4ade-ed43-21d033c3622e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "def print_unique_count(X):\n",
        "  unique, counts = np.unique(X, return_counts=True)\n",
        "  print(np.asarray((unique, counts)).astype(int).T)\n",
        "\n",
        "print('\\n\\ny_pred value_counts: \\n')\n",
        "print_unique_count(y_pred)\n",
        "print('\\n\\ny_test value_counts: \\n')\n",
        "print_unique_count(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "y_pred value_counts: \n",
            "\n",
            "[[    1 73701]\n",
            " [    2 79834]\n",
            " [    3  8906]\n",
            " [    4 18249]\n",
            " [    5 25598]\n",
            " [    6 44011]\n",
            " [    7 73035]\n",
            " [    8 68809]]\n",
            "\n",
            "\n",
            "y_test value_counts: \n",
            "\n",
            "[[    1 48912]\n",
            " [    2 49088]\n",
            " [    3 16905]\n",
            " [    4 46946]\n",
            " [    5 63466]\n",
            " [    6 56652]\n",
            " [    7 62506]\n",
            " [    8 47668]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmq5TvjWx9mY",
        "outputId": "42bcb308-520f-4ce0-a7fc-b501972917e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "_, _, X_test_final = process_train_validation(data_train, data_test, \n",
        "                                                    trainBatch=None, \n",
        "                                                    splitValSet=False,\n",
        "                                                    noTestLabel=True, \n",
        "                                                    random_state=1234)\n",
        "\n",
        "y_pred_final = clf.predict(X_test_final)\n",
        "print('\\n\\ny_pred value_counts: \\n')\n",
        "print_unique_count(y_pred_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given Train Set Shape:  (219675, 496)\n",
            "Given Validation Set Shape:  (57573, 495)\n",
            "\n",
            "X_train Shape:  (219675, 452)  |  y_train Shape:  (219675,)\n",
            "X_val Shape:  (57573, 452)\n",
            "\n",
            "Instances of each Label of the Train Set: \n",
            "1.0    43790\n",
            "2.0    36141\n",
            "5.0    35659\n",
            "7.0    29558\n",
            "8.0    26482\n",
            "6.0    20753\n",
            "4.0    20440\n",
            "3.0     6852\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    3.4s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "y_pred value_counts: \n",
            "\n",
            "[[    1  9580]\n",
            " [    2 15958]\n",
            " [    3     5]\n",
            " [    4  2874]\n",
            " [    5  7518]\n",
            " [    6  3036]\n",
            " [    7  4238]\n",
            " [    8 14364]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done 300 out of 300 | elapsed:    5.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}